{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lao Part-of-speech\n",
    "\n",
    "I use Lao corpus from https://github.com/FoVNull/SeqLabeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/FoVNull/SeqLabeling/raw/main/corpus/Lao/Lao_train.txt\n",
    "!wget https://github.com/FoVNull/SeqLabeling/raw/main/corpus/Lao_test.pkl\n",
    "!wget https://github.com/FoVNull/SeqLabeling/raw/main/corpus/Lao/Lao_valid.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pythainlp.tag import PerceptronTagger\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from itertools import chain\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Lao_test.pkl\",\"rb\") as f:\n",
    "    test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alltag = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(name):\n",
    "    with open(name ,\"r\",encoding=\"utf-8-sig\") as f:\n",
    "        _data = [i.strip() for i in f.readlines()]\n",
    "    data = []\n",
    "    for i in _data:\n",
    "        _t = []\n",
    "        _j = i.split(\"\\t\")\n",
    "        for j in _j:\n",
    "            _w, _tag = j.split(\"\\u200b\")\n",
    "            if _w==\"\":\n",
    "                _w = \"<space>\"\n",
    "            if _tag != '':\n",
    "                _t.append((_w,_tag))\n",
    "                alltag.append(_tag)\n",
    "        data.append(_t)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = readfile(\"Lao_train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = readfile(\"Lao_valid.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "alltag = list(set(alltag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x = [[i[0] for i in j] for j in valid_dataset]\n",
    "valid_y = [[i[1] for i in j] for j in valid_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"ptagger_SeqLabeling_corpus.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = PerceptronTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger.train(train_dataset, save_loc=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ບ່ອນໃດ', 'ADV'),\n",
       " ('ສາວ', 'N'),\n",
       " ('ງາມ', 'N'),\n",
       " ('ຫລາຍ', 'N'),\n",
       " ('ລະ', 'ADV'),\n",
       " ('ຮຽນ', 'V'),\n",
       " ('ໂລດ', 'N'),\n",
       " ('ບໍ່', 'NEG'),\n",
       " ('ສົນ', 'V'),\n",
       " ('ກັບ', 'PRE'),\n",
       " ('ຫຍັງ', 'DBQ'),\n",
       " ('ເລີຍ', 'ADV'),\n",
       " ('ອິໆໆໆ', 'IAC')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag(valid_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CLF',\n",
       " 'N',\n",
       " 'V',\n",
       " 'N',\n",
       " 'ADV',\n",
       " 'V',\n",
       " 'N',\n",
       " 'NEG',\n",
       " 'ADJ',\n",
       " 'PRE',\n",
       " 'NTR',\n",
       " 'ADV',\n",
       " 'IAC']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [[t for _,t in tagger.tag(sent)] for sent in valid_x]\n",
    "gold = valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(pred)):\n",
    "    if len(pred[i]) != len(gold[i]):\n",
    "        print(i)\n",
    "        print(pred[i],gold[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1154"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1154"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PUNCT',\n",
       " 'PRA',\n",
       " 'N',\n",
       " 'CNM',\n",
       " 'FIX',\n",
       " 'TTL',\n",
       " 'NTR',\n",
       " 'PRN',\n",
       " 'DMN',\n",
       " 'CLF',\n",
       " 'ONM',\n",
       " 'DBQ',\n",
       " 'COJ',\n",
       " 'IAC',\n",
       " 'ADJ',\n",
       " 'IBQ',\n",
       " 'DAN',\n",
       " 'PRE',\n",
       " 'NEG',\n",
       " 'PRS',\n",
       " 'ADV',\n",
       " 'DAQ',\n",
       " 'IAQ',\n",
       " 'PVA',\n",
       " 'V',\n",
       " 'REL']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alltag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_classification_report(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Classification report for a list of BIO-encoded sequences.\n",
    "    It computes token-level metrics and discards \"O\" labels.\n",
    "    \n",
    "    Note that it requires scikit-learn 0.15+ (or a version from github master)\n",
    "    to calculate averages properly!\n",
    "    \"\"\"\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "        \n",
    "    tagset = set(lb.classes_) - {'O'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.81      0.75      0.78       879\n",
      "         ADV       0.92      0.84      0.87       584\n",
      "         CLF       0.92      0.82      0.87       345\n",
      "         CNM       0.92      0.70      0.79        93\n",
      "         COJ       0.93      0.87      0.90       861\n",
      "         DAN       0.00      0.00      0.00         3\n",
      "         DAQ       1.00      1.00      1.00         3\n",
      "         DBQ       0.79      0.78      0.78        49\n",
      "         DMN       1.00      1.00      1.00       164\n",
      "         FIX       0.95      0.90      0.92        97\n",
      "         IAC       0.98      0.98      0.98       109\n",
      "         IAQ       1.00      1.00      1.00        10\n",
      "         IBQ       1.00      1.00      1.00         1\n",
      "           N       0.78      0.90      0.83      5108\n",
      "         NEG       0.98      0.96      0.97       207\n",
      "         NTR       0.95      0.94      0.95       149\n",
      "         ONM       0.00      0.00      0.00         6\n",
      "         PRA       0.90      0.87      0.88       439\n",
      "         PRE       0.93      0.89      0.91       935\n",
      "         PRN       0.72      0.59      0.65      1633\n",
      "         PRS       0.99      0.98      0.98       515\n",
      "       PUNCT       0.99      0.98      0.98       782\n",
      "         PVA       1.00      0.99      0.99       145\n",
      "         REL       0.88      0.93      0.91       199\n",
      "         TTL       0.82      0.77      0.80        61\n",
      "           V       0.85      0.81      0.83      3344\n",
      "\n",
      "   micro avg       0.84      0.84      0.84     16721\n",
      "   macro avg       0.85      0.82      0.83     16721\n",
      "weighted avg       0.84      0.84      0.84     16721\n",
      " samples avg       0.84      0.84      0.84     16721\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(bio_classification_report(gold, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
